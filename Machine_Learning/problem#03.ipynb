{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading news-category-dataset.zip to ./news-category-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.5M/26.5M [00:01<00:00, 25.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "dataset_url = 'https://www.kaggle.com/datasets/rmisra/news-category-dataset'\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link   \n",
       "0       https://www.huffpost.com/entry/covid-boosters-...  \\\n",
       "1       https://www.huffpost.com/entry/american-airlin...   \n",
       "2       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3       https://www.huffpost.com/entry/funniest-parent...   \n",
       "4       https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "...                                                   ...   \n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline   category   \n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS  \\\n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS   \n",
       "\n",
       "                                        short_description   \n",
       "0       Health experts said it is too early to predict...  \\\n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                     authors       date  \n",
       "0       Carla K. Johnson, AP 2022-09-23  \n",
       "1             Mary Papenfuss 2022-09-23  \n",
       "2              Elyse Wanshel 2022-09-23  \n",
       "3           Caroline Bologna 2022-09-23  \n",
       "4             Nina Golgowski 2022-09-22  \n",
       "...                      ...        ...  \n",
       "209522      Reuters, Reuters 2012-01-28  \n",
       "209523                       2012-01-28  \n",
       "209524                       2012-01-28  \n",
       "209525                       2012-01-28  \n",
       "209526                       2012-01-28  \n",
       "\n",
       "[209527 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset into the pandas dataframe\n",
    "df = pd.read_json(\"news-category-dataset/News_Category_Dataset_v3.json\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'The bottle is empty.' and 'There is nothing in the bottle.': 0.4501755023269898\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Prepare the dataset\n",
    "sentences = [\n",
    "    \"The bottle is empty.\",\n",
    "    \"There is nothing in the bottle.\",\n",
    "]\n",
    "\n",
    "# Step 2: Text preprocessing (optional, based on your requirements)\n",
    "\n",
    "# Step 3: Vectorize the sentences\n",
    "vectorizer = TfidfVectorizer()\n",
    "sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Step 4: Compute cosine similarity\n",
    "similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "# Step 5: Interpret the results\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        similarity_score = similarity_matrix[i][j]\n",
    "        print(f\"Similarity between '{sentences[i]}' and '{sentences[j]}': {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.3870967741935484\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"The bottle is empty.\"\n",
    "sentence2 = \"There is nothing in the bottle.\"\n",
    "\n",
    "# Calculate the Levenshtein Distance\n",
    "levenshtein_distance = Levenshtein.distance(sentence1, sentence2)\n",
    "\n",
    "# Calculate the similarity score\n",
    "similarity_score = 1 - (levenshtein_distance / max(len(sentence1), len(sentence2)))\n",
    "\n",
    "# Print the similarity score\n",
    "print(\"Similarity Score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Define the sentences\n",
    "sentence1 = \"The bottle is empty.\"\n",
    "sentence2 = \"There is nothing in the bottle.\"\n",
    "\n",
    "# Tokenize the sentences into sets of words\n",
    "words1 = set(sentence1.lower().split())\n",
    "words2 = set(sentence2.lower().split())\n",
    "\n",
    "# Calculate the Jaccard Similarity\n",
    "jaccard_sim = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "# Print the Jaccard Similarity value\n",
    "print(\"Jaccard Similarity:\", jaccard_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"The bottle is empty.\"\n",
    "sentence2 = \"There is nothing in the bottle.\"\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the sentences into vector representations\n",
    "sentence_vectors = vectorizer.fit_transform([sentence1, sentence2]).toarray()\n",
    "\n",
    "# Calculate the Euclidean Distance between the vectors\n",
    "euclidean_distance = distance.euclidean(sentence_vectors[0], sentence_vectors[1])\n",
    "\n",
    "# Calculate the similarity score\n",
    "similarity_score = 1 / (1 + euclidean_distance)\n",
    "\n",
    "# Print the similarity score\n",
    "print(\"Similarity Score:\", similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6123724356957946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the sentences\n",
    "sentence1 = \"The bottle is empty.\"\n",
    "sentence2 = \"There is nothing in the bottle.\"\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the sentences into vector representations\n",
    "sentence_vectors = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "# Calculate the cosine similarity between the vectors\n",
    "cosine_sim = cosine_similarity(sentence_vectors[0], sentence_vectors[1])\n",
    "\n",
    "# Print the cosine similarity value\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cosine_similarity_score'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case = lambda x: x.lower() + str(\"_score\")\n",
    "lower_case(\"cosine_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Score: 0.3666666666666667\n",
      "Jaccard Score: 0.42857142857142855\n",
      "Euclidean Score: 0.3333333333333333\n",
      "Cosine Similarity Score: 0.6123724356957946\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class SimilarityScore:\n",
    "    def __init__(self, sentence1, sentence2):\n",
    "        self.sentence1 = sentence1\n",
    "        self.sentence2 = sentence2\n",
    "    \n",
    "    def levenshtein_score(self):\n",
    "        # Calculate the Levenshtein Distance\n",
    "        levenshtein_distance = Levenshtein.distance(self.sentence1, self.sentence2)\n",
    "\n",
    "        # Calculate the similarity score\n",
    "        similarity_score = 1 - (levenshtein_distance / max(len(self.sentence1), len(self.sentence2)))\n",
    "\n",
    "        # Return the similarity score\n",
    "        return similarity_score\n",
    "\n",
    "    def jaccard_score(self):\n",
    "        # Tokenize the sentences into sets of words\n",
    "        words1 = set(self.sentence1.lower().split())\n",
    "        words2 = set(self.sentence2.lower().split())\n",
    "\n",
    "        # Calculate the Jaccard Similarity\n",
    "        jaccard_sim = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "        # Return the Jaccard Similarity value\n",
    "        return jaccard_sim\n",
    "\n",
    "    def euclidean_score(self):\n",
    "        # Create a CountVectorizer object\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        # Fit and transform the sentences into vector representations\n",
    "        sentence_vectors = vectorizer.fit_transform([self.sentence1, self.sentence2]).toarray()\n",
    "\n",
    "        # Calculate the Euclidean Distance between the vectors\n",
    "        euclidean_distance = distance.euclidean(sentence_vectors[0], sentence_vectors[1])\n",
    "\n",
    "        # Calculate the similarity score\n",
    "        similarity_score = 1 / (1 + euclidean_distance)\n",
    "\n",
    "        # Return the similarity score\n",
    "        return similarity_score\n",
    "\n",
    "    def cosine_similarity_score(self):\n",
    "        # Create a CountVectorizer object\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        # Fit and transform the sentences into vector representations\n",
    "        sentence_vectors = vectorizer.fit_transform([self.sentence1, self.sentence2])\n",
    "\n",
    "        # Calculate the cosine similarity between the vectors\n",
    "        cosine_sim = cosine_similarity(sentence_vectors[0], sentence_vectors[1])\n",
    "\n",
    "        # Return the cosine similarity value\n",
    "        return cosine_sim[0][0]\n",
    "\n",
    "# Usage example:\n",
    "sentence1 = \"The bottle is empty\"\n",
    "sentence2 = \"There is nothing in the bottle\"\n",
    "\n",
    "similarity = SimilarityScore(sentence1, sentence2)\n",
    "\n",
    "levenshtein_score = similarity.levenshtein_score()\n",
    "jaccard_score = similarity.jaccard_score()\n",
    "euclidean_score = similarity.euclidean_score()\n",
    "cosine_similarity_score = similarity.cosine_similarity_score()\n",
    "\n",
    "print(\"Levenshtein Score:\", levenshtein_score)\n",
    "print(\"Jaccard Score:\", jaccard_score)\n",
    "print(\"Euclidean Score:\", euclidean_score)\n",
    "print(\"Cosine Similarity Score:\", cosine_similarity_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    category                                  short_description\n",
       "0  U.S. NEWS  Health experts said it is too early to predict...\n",
       "1  U.S. NEWS  He was subdued by passengers and crew when he ...\n",
       "2     COMEDY  \"Until you have a dog you don't understand wha...\n",
       "3  PARENTING  \"Accidentally put grown-up toothpaste on my to...\n",
       "4  U.S. NEWS  Amy Cooper accused investment firm Franklin Te..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = df[['category', 'short_description']]\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['U.S. NEWS', 'COMEDY', 'PARENTING', 'WORLD NEWS', 'CULTURE & ARTS',\n",
       "       'TECH', 'SPORTS', 'ENTERTAINMENT', 'POLITICS', 'WEIRD NEWS',\n",
       "       'ENVIRONMENT', 'EDUCATION', 'CRIME', 'SCIENCE', 'WELLNESS',\n",
       "       'BUSINESS', 'STYLE & BEAUTY', 'FOOD & DRINK', 'MEDIA',\n",
       "       'QUEER VOICES', 'HOME & LIVING', 'WOMEN', 'BLACK VOICES', 'TRAVEL',\n",
       "       'MONEY', 'RELIGION', 'LATINO VOICES', 'IMPACT', 'WEDDINGS',\n",
       "       'COLLEGE', 'PARENTS', 'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE',\n",
       "       'HEALTHY LIVING', 'THE WORLDPOST', 'GOOD NEWS', 'WORLDPOST',\n",
       "       'FIFTY', 'ARTS', 'DIVORCE'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique categories in the dataset\n",
    "df_2['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_selector(name:str):\n",
    "    sample_list = []\n",
    "    while len(sample_list)<2:\n",
    "        sample_list.append(df_2[df_2['category']==name].sample(2)['short_description'].tolist())\n",
    "\n",
    "    sentence1 = sample_list[0]\n",
    "    sentence2 = sample_list[1]\n",
    "    similarity = SimilarityScore(sentence1[0], sentence2[0])\n",
    "\n",
    "    levenshtein_score = similarity.levenshtein_score()\n",
    "    jaccard_score = similarity.jaccard_score()\n",
    "    euclidean_score = similarity.euclidean_score()\n",
    "    cosine_similarity_score = similarity.cosine_similarity_score()\n",
    "    \n",
    "    print(f\"Category selected: {name}\\n\")\n",
    "    print(f\"Sentence1:\\n{sentence1[0]}\\n\")\n",
    "    print(f\"Sentence2:\\n{sentence2[0]}\\n\")\n",
    "\n",
    "    print(\"Levenshtein Score:\", levenshtein_score)\n",
    "    print(\"Jaccard Score:\", jaccard_score)\n",
    "    print(\"Euclidean Score:\", euclidean_score)\n",
    "    print(\"Cosine Similarity Score:\", cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category selected: COMEDY\n",
      "\n",
      "Sentence1:\n",
      "\"I’m buying 20 copies!\"\n",
      "\n",
      "Sentence2:\n",
      "Check out these Bernie DEEP CUTS.\n",
      "\n",
      "Levenshtein Score: 0.1515151515151515\n",
      "Jaccard Score: 0.0\n",
      "Euclidean Score: 0.25\n",
      "Cosine Similarity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "category_selector('COMEDY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category selected: POLITICS\n",
      "\n",
      "Sentence1:\n",
      "Much of the aid needed in the aftermath of a hurricane like Harvey would cost significantly less than the $1.6 billion proposed for wall construction.\n",
      "\n",
      "Sentence2:\n",
      "Federal authorities could reportedly announce a decision within weeks.\n",
      "\n",
      "Levenshtein Score: 0.22666666666666668\n",
      "Jaccard Score: 0.03333333333333333\n",
      "Euclidean Score: 0.1380262631157473\n",
      "Cosine Similarity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "category_selector('POLITICS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
